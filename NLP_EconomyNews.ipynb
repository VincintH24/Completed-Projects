{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed3aa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker: Economy\n",
      "Article Article 29:\n",
      "Date and Time: 2023-06-30 15:34:27\n",
      "Summary: Weekly take on events in the world economy and their fallout.\n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'POSITIVE', 'score': 0.9966251850128174}\n",
      "\n",
      "Article Article 48:\n",
      "Date and Time: 2023-07-12 09:38:07\n",
      "Summary: Headline inflation down over two-thirds since last year. Core inflation has come down much less, but is still high\n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'NEGATIVE', 'score': 0.9748467803001404}\n",
      "\n",
      "Article Article 19:\n",
      "Date and Time: 2023-07-13 20:06:34\n",
      "Summary: Director Brainard speaks at Economic Club of New York. Economy is ‘stabilising,’ Director Brainard says\n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'POSITIVE', 'score': 0.9926002621650696}\n",
      "\n",
      "Article Article 31:\n",
      "Date and Time: 2023-07-14 20:31:49.507000\n",
      "Summary: Sentiment measure now exceeds pre-pandemic levels. Democrats are more optimistic, while Republicans are less so\n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'POSITIVE', 'score': 0.9775083065032959}\n",
      "\n",
      "Article Article 26:\n",
      "Date and Time: 2023-07-17 08:35:38\n",
      "Summary: \n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'POSITIVE', 'score': 0.748120903968811}\n",
      "\n",
      "Article Article 9:\n",
      "Date and Time: 2023-07-17 10:07:00\n",
      "Summary: The U.S. economy did not enter a recession in 2022 and is not heading for one. Inflation remains above the Fed’s 2% target\n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'NEGATIVE', 'score': 0.9423289895057678}\n",
      "\n",
      "Article Article 10:\n",
      "Date and Time: 2023-07-21 18:45:55\n",
      "Summary: Consumer spending is 70% of the economy, Goldman Sachs says.\n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'POSITIVE', 'score': 0.9775075316429138}\n",
      "\n",
      "Article Article 38:\n",
      "Date and Time: 2023-07-26 10:10:34\n",
      "Summary: \n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'POSITIVE', 'score': 0.748120903968811}\n",
      "\n",
      "Article Article 7:\n",
      "Date and Time: 2023-07-27 10:10:00\n",
      "Summary: U.S. economy grew at an annual rate of 2.4% in second quarter. People have continued to spend money, on both goods and services.\n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'POSITIVE', 'score': 0.9204090237617493}\n",
      "\n",
      "Article Article 17:\n",
      "Date and Time: 2023-07-28 00:00:00\n",
      "Summary: Over 152 million Americans live in areas under some sort of heat alert. Businesses and consumers try to adapt as temperatures rise\n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'NEGATIVE', 'score': 0.9845237731933594}\n",
      "\n",
      "Article Article 11:\n",
      "Date and Time: 2023-07-28 05:28:17\n",
      "Summary: Surging business investment fuels second-quarter growth. Consumer spending slows but remains strong\n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'POSITIVE', 'score': 0.997497022151947}\n",
      "\n",
      "Article Article 2:\n",
      "Date and Time: 2023-07-28 06:30:16\n",
      "Summary: \n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'POSITIVE', 'score': 0.748120903968811}\n",
      "\n",
      "Article Article 5:\n",
      "Date and Time: 2023-07-28 14:58:53\n",
      "Summary: The market’s rate cut expectations for the coming year are at this point.\n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'NEGATIVE', 'score': 0.7004550695419312}\n",
      "\n",
      "Article Article 44:\n",
      "Date and Time: 2023-07-28 17:00:25\n",
      "Summary: The United States is putting Europe to shame. Optimism about the economy is at its highest since 2022\n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'POSITIVE', 'score': 0.993433952331543}\n",
      "\n",
      "Article Article 27:\n",
      "Date and Time: 2023-07-29 06:00:27\n",
      "Summary: Coca-Cola, Hilton, Visa all boost earnings forecasts this week.\n",
      "\n",
      "Sentiment Analysis:\n",
      "{'label': 'POSITIVE', 'score': 0.894982635974884}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from dateutil.parser import parse\n",
    "import pytz\n",
    "import datetime\n",
    "# Define the name of the pre-trained Pegasus model for financial summarization\n",
    "model_name = \"human-centered-summarization/financial-summarization-pegasus\"\n",
    "\n",
    "# Initialize the tokenizer for the Pegasus model\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the pre-trained Pegasus model for conditional text generation\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Define the URL to be used for web scraping financial news related to the US economy\n",
    "url = \"https://www.google.com/search?sxsrf=APwXEdfoORN23DmKxvNwhGnPcYR7KYaVng:1686905788002&q=us+economy&tbm=nws&sa=X&ved=2ahUKEwiWuY3itcf_AhUDkYkEHYCHA2EQ0pQJegQIBxAB&biw=1920&bih=872&dpr=2\"\n",
    "\n",
    "# Send an HTTP GET request to the URL and store the response\n",
    "r = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# Extract all paragraph elements from the HTML content\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "# Define a list of monitored tickers (in this case, only 'Economy')\n",
    "monitored_tickers = ['Economy']\n",
    "\n",
    "# Function to search for news URLs related to a given ticker\n",
    "def search_for_stock_news_urls(ticker):\n",
    "    urls = []\n",
    "    for page in range(5):  # Change the number of pages here\n",
    "        start_index = page * 10\n",
    "        search_url = f\"https://www.google.com/search?sxsrf=APwXEdfoORN23DmKxvNwhGnPcYR7KYaVng:1686905788002&q=us+economy{ticker}&tbm=nws&start={start_index}\"\n",
    "        r = requests.get(search_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        atags = soup.find_all('a')\n",
    "        hrefs = [link['href'] for link in atags]\n",
    "        urls.extend(hrefs)\n",
    "    return urls\n",
    "\n",
    "# Create a dictionary that maps each ticker to a list of URLs containing news articles related to that ticker\n",
    "raw_urls = {ticker: search_for_stock_news_urls(ticker) for ticker in monitored_tickers}\n",
    "\n",
    "# Define a list of words to exclude from the URLs\n",
    "exclude_list = ['maps', 'policies', 'preferences', 'accounts', 'support']\n",
    "\n",
    "# Function to clean the URLs by removing unwanted ones based on specific criteria\n",
    "def strip_unwanted_urls(urls, exclude_list):\n",
    "    val = []\n",
    "    for url in urls:\n",
    "        if 'https://' in url and not any(exclude_word in url for exclude_word in exclude_list):\n",
    "            res = re.findall(r'(https?://\\S+)', url)[0].split('&')[0]\n",
    "            val.append(res)\n",
    "    return list(set(val))\n",
    "\n",
    "# Create a dictionary that maps each ticker to a list of cleaned URLs\n",
    "cleaned_urls = {ticker: strip_unwanted_urls(raw_urls[ticker], exclude_list) for ticker in monitored_tickers}\n",
    "\n",
    "# Function to scrape and process the content of the URLs\n",
    "def scrape_and_process(URLs):\n",
    "    ARTICLES = []\n",
    "    for i, url in enumerate(URLs):\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        # Extract date and time information from the article\n",
    "        datetime_element = soup.find('time')\n",
    "        if datetime_element and 'datetime' in datetime_element.attrs:\n",
    "            datetime_str = datetime_element.get('datetime')\n",
    "            article_datetime = parse(datetime_str).replace(tzinfo=None)\n",
    "        else:\n",
    "            article_datetime = None\n",
    "\n",
    "        # Extract paragraphs from the article and limit the text to 350 words\n",
    "        paragraphs = soup.find_all('p')\n",
    "        text = [paragraph.text for paragraph in paragraphs]\n",
    "        words = ' '.join(text).split(' ')[:350]\n",
    "        ARTICLE = ' '.join(words)\n",
    "\n",
    "        # Add article and datetime to the list of articles\n",
    "        ARTICLES.append((f\"Article {i+1}\", ARTICLE, article_datetime))\n",
    "\n",
    "    # Sort articles by datetime in ascending order\n",
    "    ARTICLES.sort(key=lambda x: x[2] if x[2] is not None else datetime.datetime.min)\n",
    "\n",
    "    return ARTICLES\n",
    "\n",
    "# Create a dictionary that maps each ticker to a list of processed articles\n",
    "articles = {ticker: scrape_and_process(cleaned_urls[ticker]) for ticker in monitored_tickers}\n",
    "\n",
    "# Initialize an empty list to store filtered articles for each ticker\n",
    "filtered_articles = []\n",
    "\n",
    "# Loop over each ticker to filter articles with valid datetime information\n",
    "for ticker in monitored_tickers:\n",
    "    ticker_articles = articles[ticker]\n",
    "    filtered_ticker_articles = []\n",
    "\n",
    "    for article_number, article, datetime in ticker_articles:\n",
    "        if datetime is not None:\n",
    "            filtered_ticker_articles.append((article_number, article, datetime))\n",
    "\n",
    "    filtered_articles.append(filtered_ticker_articles)\n",
    "\n",
    "# Function to summarize each article using the Pegasus model\n",
    "def summarize(articles):\n",
    "    summaries = []\n",
    "    for article_number, article, _ in articles:\n",
    "        input_ids = tokenizer.encode(article, return_tensors='pt')\n",
    "        output = model.generate(input_ids, max_length=55, num_beams=10, early_stopping=True)\n",
    "        summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        summary = summary.replace(\"We are aware of the issue and are working to resolve it.\", \"\")\n",
    "        summary = summary.replace(\"All images are copyrighted.\", \"\")\n",
    "        summary = summary.replace(\"Find the best credit cards, loans, insurance and more in SELECT.\", \"\")\n",
    "        summaries.append(summary)\n",
    "    return summaries\n",
    "\n",
    "# Create a list to store summarized articles for each ticker\n",
    "summarized_articles = []\n",
    "\n",
    "# Loop over each ticker to summarize its filtered articles\n",
    "for ticker_articles in filtered_articles:\n",
    "    summarized_ticker_articles = summarize(ticker_articles)\n",
    "    summarized_articles.append(summarized_ticker_articles)\n",
    "\n",
    "# Initialize a sentiment analysis pipeline using the Hugging Face Transformers library\n",
    "sentiment = pipeline('sentiment-analysis')\n",
    "\n",
    "# Loop over each ticker to print the summarized articles along with their sentiment analysis results\n",
    "for ticker, ticker_articles, summarized_ticker_articles in zip(monitored_tickers, filtered_articles, summarized_articles):\n",
    "    print(f\"Ticker: {ticker}\")\n",
    "    for (article_number, article, datetime), summary in zip(ticker_articles, summarized_ticker_articles):\n",
    "        print(f\"Article {article_number}:\")\n",
    "        print(f\"Date and Time: {datetime}\")\n",
    "        print(f\"Summary: {summary}\")\n",
    "        print()\n",
    "        sentiment_results = sentiment([summary])\n",
    "        print(\"Sentiment Analysis:\")\n",
    "        for result in sentiment_results:\n",
    "            print(result)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc915948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ModelTwo",
   "language": "python",
   "name": "modeltwo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
